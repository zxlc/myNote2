
# 数据结构与算法问题

## 数组

#### 二分查找

- 前提是有序数组
- 从中间位置元素开始，若中间位置的元素就是目标元素，则返回；若目标元素小于中间位置元素，则进入左子区查找；若大于中间位置元素，则进入右子区查找。在左子区或右子区查找时是跟前面一样的逻辑。
- 二分查找的时间复杂度是O(logn)

#### 快速排序

分治与递归的思想
- 从数组中选出一个数据认为是已排好序的
- 将小于该数据的元素放在左边，大于或等于该数据的元素放在其右边
- 然后将左右两个子数列通过递归的方式进行同样的操作，直到数列中只有一个元素为止

- 时间复杂度：O(nlogn)
- 空间复杂度：以递归方式使用了系统调用栈，O(logn)

## 链表

#### 链表是否有环

1. 哈希表：遍历链表，使用一个HashSet记录访问过的结点，在到达空结点之前如果有结点被重复访问，则说明链表有环
    - 时间复杂度：O(n)
    - 空间复杂度：O(n)

2. 快慢指针：两个指针同时遍历链表，慢指针一次移动一个元素，快指针一次移动两个元素，若无环，则快指针先到达空结点，否则两个指针都进入环后会有相遇的时候，若同时指向同一个结点，说明有环
    - 时间复杂度：O(n)
    - 空间复杂度：O(1)

#### 反转单链表

循环遍历将每一个结点的next指针指向其父节点，需要注意的点是：
- 单链表无法通过当前结点访问其前一个结点，所以循环体之外需要一个变量记录当前结点的前一个结点
- next指向改变之后原链表就断开了，所以循环体之内，改变next指针的指向之前，需要先保留原next结点

循环结束之后记录当前结点前一个结点的指针会指向新链表的头结点

#### LRU缓存淘汰算法（双向链表+哈希表）

LRU，这种算法认为最近使用的数据是热门数据，下一次很大概率将会再次被使用。而最近很少被使用的数据，很大概率下一次不再用到。当缓存容量的满时候，优先淘汰最近最少使用的数据。

LruCache是Android的实现类，其内部使用了 LinkedHashMap，LinkedHashMap 内部使用的是 HashMap + 双向链表，HashMap用于存储数据，双向链表用于记录结点的插入和访问顺序，添加、删除及移动等操作的时间复杂度为 O(1)。

1. 添加/删除数据如何操作？

    新添加的数据会放到链表末尾，然后检查缓存大小是否超出容量限制，若超出则从链表的头结点删除数据

2. 访问数据如何操作？

    先通过key从HashMap中获取数据，然后刚访问的数据会被移到链表的末尾

3. 为什么使用HashMap？

    HashMap可以通过key直接访问到结点，是访问的时间复杂度为 O(1)

4. 双向链表是如何实现的？

    LinkedHashMap 对 HashMap 的 Node类 做了扩展，在原结构的基础上增加了前一个结点与后一个结点的引用

5. 为什么使用双向链表，而不是单链表？

    移动结点时需要操作其前一个结点，添加结点时需要操作链表的尾结点，双向链表这两个操作的时间复杂度是1，而单链表需要通过遍历才能访问到这两个位置，其时间复杂度为 n，效率会被大大降低


## 队列&栈

#### 栈实现队列

两个栈实现队列：
- 添加元素：正常向栈中添加元素
- 取出元素：弹出栈底元素上方的所有元素，依次放入另一个栈中，待弹出栈底元素后，再将另一个栈中的元素放回原栈中

#### 队列实现栈

两个队列实现栈：
- 添加元素：正常向队列中添加元素
- 取出元素：将原队列中的最后一个元素前面的所有元素依次放入另一个队列中，待将最有一个元素出队后，再将另一个队列中的元素放回原队列中

#### 判断括号字符串是否有效

[https://leetcode.cn/problems/valid-parentheses/](https://leetcode.cn/problems/valid-parentheses/)

通过栈的来实现：
- 遍历字符串，遇到左括号就将其入栈
- 每遇到一个右括号，就从栈中弹出一个元素，并判断弹出元素与右括号是否对应，若对应则继续遍历，若不对应则不是有效的括号字符串


## 堆、优先队列

#### 数据流的第K大数值

[https://leetcode.cn/problems/jBjn9C/](https://leetcode.cn/problems/jBjn9C/)

使用最小堆来过滤：
- 从数据流中取出k个数建立最小堆，将剩余的数据与堆顶比较，如果比堆顶大，则替换堆顶，这样最后堆中的元素就是最大的k个数
- 时间复杂度：O(nlogk)

#### 优先队列底层实现

PriorityQueue类内部使用了最小堆，最小的元素会被放在堆顶，优先级最高

使用优先队列时可以指定堆的大小和比较器


## HashMap

https://www.cnblogs.com/java1024/p/13488714.html

https://juejin.cn/post/6844904013909983245

https://www.jianshu.com/p/786521c007f2

#### HashMap原理介绍

哈希表，通过将关键码值映射到表中的一个位置来直接进行访问，以提升查找速度

#### 哈希函数

key与存储位置的对应关系，要求计算简便高效、哈希值均匀分布
> 在真实情况下几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径来解决

#### 哈希冲突解决？

1. 开放寻址法：如果出现散列冲突，需要重新探测一个空闲位置，将其插入。但这种方法并不常用，因为相对复杂且局限性大，一般用于小数据量的情况，Java中的 ThreadLocalMap 用的是这种方法

2. 链表法：在冲突处构建链表，将冲突的元素加入链表，如果遭到恶意哈希碰撞，哈希表会退化为链表，所有元素被存储到同一个链表中，此时哈希表的查找效率等同于链表，时间复杂度为O(logn)

3. 再哈希法：准备多个哈希函数，当一个哈希函数计算结果冲突时，再使用其他函数计算直到没有冲突，这种方法元素不会聚集，但增加了计算时间

4. 建立公共溢出区：将哈希表分为公共表和溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。

#### put方法执行过程

添加数据：
1. 检查数组是否为空，若为空先初始化数组
2. 计算key的hash值与数组索引
3. 插入元素：
    - 若索引位置没有结点，则创建新结点，直接存储到该位置
    - 若索引位置的结点的key与新添加key相同，则替换结点值
    - 若索引位置结点为红黑树，则将新元素插入到树中
    - 否则索引位置的结点为链表，若链表中key已存在则替换其值，否则将新元素插入到链表中，并检查链表长度，若>=8则转为红黑树
4. 检查元素数量是否超过阈值（负载因子*数组容量），若超过则进行扩容


扩容：

5. 数组容量变为原来的2倍
6. 重新计算各元素在数组中的位置，将其存储到新数组中

![](https://img-blog.csdnimg.cn/20200618150149962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTQxNzcz,size_16,color_FFFFFF,t_70#pic_center)

#### HashMap在JDK1.7和JDK1.8中有哪些不同？
1. 底层数据结构：1.7数组+链表，1.8数组+链表/红黑树
2. 链表的插入方式：从头插法改成了尾插法，扩容后链表中元素的相对位置不变
3. 扩容时元素位置重新确定：
    - 1.7重新计算元素位置
    - 1.8逻辑更简单，通过判断hash值余数部分新增的bit位是0还是1，确定元素位置不变或向后移动原数组长度个位置
4. 添加数据时，1.7先扩容再插入，1.8先插入再扩容

#### 1.8为什么引入红黑树？

当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。

#### 负载因子是什么？为什么默认值是0.75?

负载因子是装载元素数量的一个限制值，范围是0~1，当元素数目超过负载因子与容量的乘积时，HasHMap就会扩容。如果该值设置的过小，扩容就会比较频繁，会浪费内存，如果过大，内存利用高，但哈希冲突概率也会增加，会导致查询效率变低，0.75应该是考虑到空间成本与时间成本的一个折中值。

#### HashMap的容量设置

HashMap默认容量是16，这个值是可以设置的，如果事先知道大概的数据量，可以通过修改默认容量减少扩容次数，这样会提高HashMap的性能。

#### 为什么数组容量是2的幂次方，扩容时以2倍扩容？

设置成2的幂次方后，计算元素位置的取模运算可以简化成 hash值与数组长度-1的与运算，与运算比取模运算的效率要高，可以提升HashMap的性能

#### HashMap是线程安全吗？HashMap与Hashtable区别？

1. HashMap不是线程安全的：
由于JDK1.7的HashMap向链表中插入元素采用的是头插法，在多线程情况下会存在死循环问题，JDK1.8已经改成了尾插法，不存在这个问题了，但还有其他的并发修改问题。

2. Hashtable是遗留类，它是线程安全的，但并发性不如 ConcurrnetHashMap，因为ConcurrentHashMap引入了分段锁

#### HaspMap中key的要求

- 同一个key，多次计算hashCode其值应该相同
- 一般选择为String作为key，如果要自定义对象，需要重写hashCode和equals方法

#### 红黑树

红黑树是能够自平衡的二叉搜索树，它为每个结点增加了颜色属性，并要求结点颜色满足一些性质来使树的高度达到相对平衡，从而保证树的插入、删除、查找操作的时间复杂度为 O(log n)。

当插入或删除元素树的平衡被破坏之后，红黑树会通过旋转结点和调整结点的颜色来维持平衡

## 二叉树

#### 遍历方式

- 深度优先：
    - 有递归和栈两种方式，
    - 先序遍历：根 - 左 - 右
    - 中序遍历：左 - 根 - 右（二叉搜索树）
    - 后序遍历：左 - 右 - 根
    - 使用场景：寻找到某个结点的路径
- 广度优先：
    - 使用队列，按照层级遍历，先遍历根节点，然后是一级子结点，二级子结点 。。。
    - 使用场景：最短路径

## 算法思想

[https://blog.51cto.com/u_1661518/1396943](https://blog.51cto.com/u_1661518/1396943)

分治策略：
- 用递归的方式将原问题拆分成规模较小的子问题，然后合并子问题的解建立原问题的解
- 例题：
    - 快速排序、归并排序
    - 合并K个排序链表
    - [https://leetcode.cn/tag/divide-and-conquer/problemset/](https://leetcode.cn/tag/divide-and-conquer/problemset/)

动态规划：
- 动态规划与分治策略类似，不同的是，动态规划会将子问题的解存储起来，再次求解时直接拿来使用。动态规划求解的问题是分治策略的子集，但动态规划运行时间更短。
- 例题：
    - 最长回文子串
    - 最大子数组和
    - 杨辉三角
    - 斐波那契数列
    - [https://leetcode.cn/tag/dynamic-programming/problemset/](https://leetcode.cn/tag/dynamic-programming/problemset/)

贪心算法：
- 贪心算法在每一步都做出最优的选择，希望这样能得到全局最优解，但不能保证最后一定是最优解。
- 贪心选择性质：我们可以通过做出局部最优（贪心）来构造全局最优。只要我们能够证明该问题具有贪心选择性质，就可以用贪心算法对其求解。
- 例题：
    - 数组拆分
    - [https://leetcode.cn/tag/greedy/problemset/](https://leetcode.cn/tag/greedy/problemset/)

总结：
- 分治策略一般用于解决子问题相互独立的情况，一般用递归实现
- 动态规划用于解决子问题有重叠的情况，既可以用递归实现，也可以用迭代实现
- 贪心选择用于解决具有贪心选择性质的一类问题，既可以用递归实现，也可以用迭代实现，因为很多递归贪心算法都是尾递归，很容易改成迭代贪心算法。


#### 算法题

两数之和 [https://leetcode.cn/problems/two-sum/](https://leetcode.cn/problems/two-sum/)

有效的字母异位词 [https://leetcode.cn/problems/valid-anagram/](https://leetcode.cn/problems/valid-anagram/)


## ConcurrentHashMap 线程安全实现

[https://juejin.cn/post/6844903813892014087#heading-10](https://juejin.cn/post/6844903813892014087#heading-10)

Collections.SynchronizedMap 与 Hashtable 使用的是全局锁，会导致严重的性能问题

#### JDK 1.7 实现

JDK 1.7 中 ConcurrentHashMap 使用了分段锁：

- ConcurrentHashMap 维护了一个 Segment 数组，Segment 继承自 ReentrantLock，每个 Segment 都会维护一个哈希表（HashEntry 数组）
- put 数据时根据 hash 值确定属于哪个 Segment，然后先获取 Segment 锁，通过 Segment 将数据添加进哈希表
- 扩容时只针对每个 Segment 中的哈希表进行扩容

#### JDK 1.8 实现

JDK 1.8 的 ConcurrentHashMap 不再使用 Segment，而是全局只有一个哈希表，存储结构跟 HashMap 类似。ConcurrentHashMap 的线程安全采用了 CAS + synchronized 方式：

- synchronized方式：在修改链表或树的结构时（添加/删除数据）、扩容重新计算结点的位置时会先获取链表头结点的锁
- CAS方式：ConcurrentHashMap 中的 CAS 是使用 Unsafe 类实现的，主要是在初始化哈希表与扩容时通过检查或修改一些共享变量来保证线程安全

#### 1.7 与 1.8 对比

- 查询效率：1.8 引入红黑树提升了查询效率
- 并发性能：1.7 中是根据结点的哈希值分段加锁的，每个锁锁定的是一个区间的哈希值，1.8 则是对链表的头结点加锁，每个头结点的哈希值都不同，锁的粒度更小，可以支持更多线程的并发操作，1.8 还使用了 CAS，在保证线程安全的同时可以省去线程在阻塞与运行两个状态切换时间

#### 为什么要引入CAS?

ConcurrentHashMap 在初始化哈希表和判断是否需要扩容时是通过管理共享变量来保证线程安全的，CAS 机制在共享变量的原子性操作上效率比锁机制更高

#### 为什么要用 synchronized，CAS 不是已经可以保证操作的线程安全吗？

- CAS机制只能保证共享变量操作的原子性，而不能保证代码块的原子性

## 原子类

[https://blog.csdn.net/weixin_45124488/article/details/115200512](https://blog.csdn.net/weixin_45124488/article/details/115200512)
